\relax 
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\select@language{french}
\@writefile{toc}{\select@language{french}}
\@writefile{lof}{\select@language{french}}
\@writefile{lot}{\select@language{french}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Description du probl\IeC {\`e}me}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Int\IeC {\'e}r\IeC {\^e}t}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Formalisation}{1}}
\citation{maas-EtAl:2011:ACL-HLT2011}
\@writefile{toc}{\contentsline {section}{\numberline {2}Donn\IeC {\'e}es}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}M\IeC {\'e}thodologie}{2}}
\citation{Salton:1986:IMI:576628}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Exemple d'un commentaire n\IeC {\'e}gatif et d'un commentaire positif extrait de l'ensemble d'apprentissage.}}{3}}
\newlabel{example_imdb}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Premi\IeC {\`e}res tentatives}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sac de mots}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Description}{3}}
\citation{Salton:1986:IMI:576628}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Apprentissage}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pr\IeC {\'e}cisions des sacs de mots avec diff\IeC {\'e}rents algorithmes (BOW : sacs de mots, TF-IDF : sacs de mots pond\IeC {\'e}r\IeC {\'e}s par IDF)}}{4}}
\newlabel{results_bow}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Matrice de confusion de la r\IeC {\'e}gression logistique et des for\IeC {\^e}ts al\IeC {\'e}atoires pour des sacs de mots.}}{5}}
\newlabel{confusion_matrices_bow}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Repr\IeC {\'e}sentation d'une r\IeC {\'e}gression logistique sous forme de r\IeC {\'e}seau.}}{5}}
\newlabel{lr_net}{{4}{5}}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\citation{van2008visualizing}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Les 10 mots ayant les poids associ\IeC {\'e}s les plus n\IeC {\'e}gatifs}}{6}}
\newlabel{negative}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Les 10 mots ayant les poids associ\IeC {\'e}s les plus positifs}}{6}}
\newlabel{positive}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Vecteurs de mots}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Description}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces R\IeC {\'e}seau de neurones du mod\IeC {\`e}le Skip-gram (source : Wikimedia).}}{7}}
\newlabel{skip_gram}{{7}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Projection des vecteurs de mots en 2D en utilisant l'algorithme t-SNE.}}{8}}
\newlabel{tsne_plot}{{8}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Apprentissage}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Conclusion}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Un cluster contenant des mots li\IeC {\'e}s au mot du cin\IeC {\'e}ma et un autre contenant des mots-outils.}}{10}}
\newlabel{clusters}{{9}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Pr\IeC {\'e}cisions des vecteurs de mot avec diff\IeC {\'e}rents algorithmes (Vec : vecteurs de mots, TF-IDF : vecteurs de mot pond\IeC {\'e}r\IeC {\'e}s par IDF)}}{10}}
\newlabel{results_vec}{{10}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \IeC {\'E}volution de la pr\IeC {\'e}cision en fonction de la taille de l'ensemble d'apprentissage avec des sacs de mots en entr\IeC {\'e}e (en haut) et des vecteurs de mots (en bas).}}{11}}
\newlabel{learning_curves}{{11}{11}}
\citation{le2014distributed}
\citation{le2014distributed}
\citation{le2014distributed}
\@writefile{toc}{\contentsline {section}{\numberline {4}Prise en compte de l'ordre des mots}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}N-grammes}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Pr\IeC {\'e}cisions des sacs de n-grammes en entr\IeC {\'e}e d'une r\IeC {\'e}gression logistique.}}{12}}
\newlabel{results_ngram}{{12}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Vecteurs de paragraphe}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Framework d'apprentissage des paragraph vectors (source : Mikolov et al. \cite  {le2014distributed}).}}{13}}
\newlabel{paragraph_vectors}{{13}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Les 10 mots les plus semblables \IeC {\`a} 'interesting' pour un mod\IeC {\`e}le PV-DM avec moyenne.}}{14}}
\newlabel{similarities}{{14}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Pr\IeC {\'e}cision des paragraph vectors avec les deux variantes utilis\IeC {\'e}es pour des vecteurs de dimension 300}}{14}}
\newlabel{results_pv}{{15}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Courbe ROC pour le mod\IeC {\`e}le PC-DM avec concat\IeC {\'e}nation.}}{14}}
\newlabel{pcdmconcat}{{16}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Courbe ROC pour le mod\IeC {\`e}le PC-DM avec moyenne.}}{15}}
\newlabel{pcdmmean}{{17}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Derniers essais}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{15}}
\bibstyle{plain}
\bibdata{sentiment_analysis}
\bibcite{le2014distributed}{1}
\bibcite{maas-EtAl:2011:ACL-HLT2011}{2}
\bibcite{mikolov2013efficient}{3}
\bibcite{mikolov2013distributed}{4}
\bibcite{Salton:1986:IMI:576628}{5}
\bibcite{van2008visualizing}{6}
